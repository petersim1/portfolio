---
title: Let's Play Blackjack
date: 2023-12-15
excerpt: "Overview of a Q Learning approach to Blackjack"
tags:
  - machine learning
  - reinforcement learning
  - blackjack
---

I've always been interested in the game of Blackjack and understanding how "optimal" certain moves are that you can find in a optimal strategy chart. I wondered if I could apply machine learning to Blackjack in order to understand the game more deeply, while creating agents to help infer optimal gameplay. If you go to a casino, the dealer is obliged to tell you the best move at a given point in time. But let's dive deeper...

<MdImage
  src="https://www.blackjack.com.au/wp-content/uploads/2014/12/Simple_Blackjack_Strategy.png"
  caption="An example of an optimal strategy chart"
/>

## The game of Blackjack

Blackjack is a game played against the house. The objective is to get a higher card total than the house, without going over 21. At a given point in time, depending on the cards at play, a player can <mark>hit, stay, double, split, surrender</mark>. For the purpose of this analysis, I don't care about <mark>insurance</mark> or any other side bets. In blackjack, possible actions are conditional on the state that you are in, and the number of moves you have already taken. For example, after electing to hit, you can no longer double, surrender, or split, so the action space changes.

After the players are finished with their round, the house draws cards until they have at least 17. Once the house is finished, rewards are distributed accordingly. A caveat is that if the house has natural blackjack, then the players cannot move and they lose immediately, unless the player also has natural blackjack, then they push.

Here's how results are determined:
- **player $\gt$ 21** : player loses
- **21 $\ge$ player $\gt$ house** : player wins
- **player $=$ house** : push
- **player $\lt$ house $\le$ 21** : player loses
- **player surrenders** : player forfeits half their wager


Depending on which environment you play in, the rules might differ a bit. I'll outline common rules that I elected to use for my analysis, although these can be adjusted easily:
<Table
  header={["Rule", "Default", "Description"]}
  rows={[
    ["Dealer hits soft 17", "false", ""],
    ["Push on Dealer 22", "false", "some low minimum tables have this..."],
    ["Double After Split", "true", ""],
    ["Hit After Splitting Aces", "false", ""],
    ["Blackjack Payout", "3 / 2", ""],
    ["Allow Surrender", "true", ""],
    ["Split any Ten", "true", "whether you can split any cards with a value of 10"]
  ]}
  caption="Allowed Rules and their default values"
/>

I also state that we use 6 decks, and cards are not replaced until a "stop card" is reached, which is 66.67% of the way through the deck.

## Reinforcement Learning

Reinforcement learning is an application of machine learning where we're specifically tasked with maximizing some cumulative reward from a series of states and actions taken.

Think of trying to create an agent that moves through a maze. We don't have “ground truth” or direct “labels” about what the immediate action should be. However, we know that if the agent completes the maze efficiently without human interference, then it was properly trained. Maybe each step receives a reward of -1, and reaching the goal receives a reward of 10. Surely, reaching the goal in the least number of steps leads to the highest cumulative reward. But how do we train this, since we don't observe immediate feedback from a given action? That's where Reinforcement Learning comes in.

In the game of blackjack, we have an environment. Let's think of this as the rules of the game, the objective, and generally just the boundaries of what's possible in the game. At each point during the game, we observe a state. Simply put, this includes a player's current cards and the face-up house card (let's assume we have no idea what the card count is, as we easily forget what cards we've seen previously). We are presented with a policy of possible actions to take. What is our optimal action according to this policy? Can we learn what the optimal action should be? We don't necessarily care what the immediate action is, we simply care about maximizing our reward from the series of actions taken. Maybe we take a seemingly sub-optimal immediate action, because it actually leads to a higher long term reward.

### Q learning

Blackjack is an episodic task; observe a state, take an action, receive a reward, and repeat until the gameplay ends, meaning we've reached a terminal state. At each step, given a current state, we can take a specific action that is available in that state. Upon each action, we receive some reward. In some instances, we can observe the probability of moving to that next state, given the action taken, which we call a transition probability. In other instances, we don't know this transition probability, so we can consider this model-free learning. In this blackjack reinforcement learning agent, I'll use a model-free approach, meaning if we want to take an action, we are certain that we will take it.

Traditionally in Reinforcement Learning, we can observe the Value of being in a given state. Ideally, we'd want to take an action that leads to the maximum Value of the next state. Newer approaches abstract this further, and introduce the concept of Q values, which denote a measure of state-action pairs. So, rather than simply observing Value of a state, we observe a Q value of a state-action pair, and can selection our action accordingly.

Through reinforcement learning, we aim to learn to the Q function through iterative approaches.

$s$ = current state\
$s'$ = next state\
$a$ = current action\
$s'$ = next action\
$Q$ = q value\
$lr$ = learning rate\
$\gamma$ = discount factor. How important future actions are (0, 1)

```math
\begin{aligned}
Q(s,a) = Q(s,a) + lr * \lbrack R + \gamma * max(Q(s',a')) - Q(s,a)\rbrack
\end{aligned}
```

At each state $s$, we take an action $a$, receive a reward $R$, and end up in state $s'$. Assuming that $s'$ is not a terminal state, we can evaluate the optimal $Q$ value in this new state as well (if it is terminal, we'll assume it's $0$). Essentially, we are taking a weighted average of the current $Q$ value and new information. We'll initialize all Q values to $0$.

Really, this is called the SARSA algorithm (State, Action, Reward, State (next), Action (next)).

This requires storing the Q values of all state-action pairs in memory. In our case of Blackjack, without accounting for card count, this is a tractable problem computationally, which is why simple SARSA Q learning is a valid approach.

The “states” I use are: <mark>Player Card Total, House Card Shown, Useable Ace</mark>. The “actions” are: <mark>Hit, Stay, Double, Split, Surrender</mark>. Each pair of these will have an associated Q-value. As mentioned earlier, not every action is feasible given the current state, so these unreachable states are “masked” given the valid moves determined by the Player module.

```json
Q[(10, 4, false)] = {
  "hit": 0,
  "stay": 0,
  "double": 0,
  "split": 0,
  "surrender": 0,
}
```

### Exploration vs. Exploitation

Exploration vs. Exploitation is a core problem of reinforcement learning. How much new exploration do we do of the space, versus how much do exploit the current knowledge of what we know is “good” or “bad”. There are ways around this during the training process, such as :

- **Greedy policies**
  - We always take the action that we know is best given our current information.
  - Prevents exploration.
- **$\epsilon$-Greedy policies**
   - Each episode, with probability $\epsilon$, we randomly take an action, otherwise we take the best action.
   - Through learning, we can decay the epsilon value, such that we transition from high exploration initially, to more exploitation later in learning.
- **Posterior Sampling**:
  - Each episode, we sample from the Q-space to determine an action.
  - State-action Q values that are “good” are more likely to be selected, but we never allow for zero probability of selecting a “less good” action given the policy and the current state.


## My Approach

I created some extensible python modules to be able to play this game. It consists of 3 main modules, which are the <mark>Cards, Player, Game</mark> modules. I won't go into the detail of these, but we're able to efficiently dictate gameplay for 1 to many players.

Next, I wrote some helper functions to enable training and inference. Some of these include functions to generate episodes, select best actions, learn a policy, and accumulate rewards from $N$ games and $M$ rounds per game.

I assume that only 1 Player is used during training, who wagers 1 unit each round. I haven't evaluated the training procedure for more than 1 player at a time, but since each player independently plays against the house, I can't imagine there is much difference.

I train for $5,000,000$ rounds, use $\gamma = 0.99$, decayed $lr$, $\epsilon$-Greedy action selection, where $\epsilon$ is decayed. I evaluate every $10,000$ rounds, where I assess $50$ games of $100$ rounds each, and accumulate average rewards.

To dictate the decay factor, I use the following:
```python
def get_decay_factor(max_val, min_val, n):
    return np.log(max_val / min_val) / n

def get_current_decay_value(max_val, decay_factor, n):
    return max_val * np.exp(-decay_factor * n)

eps_decay = get_decay_factor(1, 0.1, n_episodes)
lr_decay = get_decay_factor(0.1, 0.001, n_episodes)

for i in range(n_episodes):
  lr = get_current_decay_value(0.1, lr_decay, i)
  eps = get_current_decay_value(1, eps_decay, i)
```

### Action Masking

In order to properly sample from the Q space and determine the optimal policy, each player's turn has the action space masked according to their current state. For example, “double” is only available as a first move. This means that after the player's first move, “double” is masked in the action space and will never be sampled from or be selected as an optimal play. Doing this, versus adding additional partitions to the state-action pairs, allows me to share more information across iterations.

```python
state = (10, 4, false)
Q[state] = {
  "hit": 0,
  "stay": 0,
  "double": 0,
  "split": 0,
  "surrender": 0,
}
policy = ["hit", "stay"]
print(applyMask(Q, policy))
--------------------------
{
  "hit": 0,
  "stay": 0,
}
```

For example, I've seen implementations where the state-action pair includes <mark>player_total, house_shows, useable_ace, can_split</mark>. Actually, this was my first implementation as well. But abstracting away <mark>can_split</mark>, and using action masking instead, led to more efficient training and more shared information between states.

Consider infrequent states, such as having a pair. We'll use a pair of 7's, for example. If we used <mark>can_split</mark> as a state, we'd have $(14, house, false, true)$. We'd have very little knowledge about the other actions, as this state is so so infrequently visited. Abstracting away <mark>can_split</mark>, and using action masking, when a pair of 7's is observed, we can borrow information about <mark>hit, stay, double, surrender</mark> for all card combinations that give us $14$.

### Training

We can use monte-carlo methods to simulate gameplay and learn an optimal policy in each iteration. So, Q values are updated every iteration, but the learned policy is only evaluated every $50,000$ iterations, to speed up training.

During each evaluation period, we can aggregate average rewards, but also percent of correct moves compared to a baseline "optimal strategy" that I found online. I partition these percent correctness scores by <mark>Hard Total, Soft Totals, Splits</mark>, so I can truly quantify where the model is performing worst. There are many intricacies that I'm leaving out, but they can all be found in the code in my github.

Some states are visited very infrequently. Empirically I found that Q values didn't converge for these. What I ended up doing is finding a aggregate of cards that were visited infrequently (basically just pairs and soft totals), and I force the Player hand to be initialized with these cards every 10 iterations. This at least seems to help with soft + split partitions approach the baseline performance more feasibly.

<MdImage
  src="/images/posts/q-learning-training-r.png"
  caption="Average rewards through training"
/>

<MdImage
  src="/images/posts/q-learning-training-p.png"
  caption="Percent correct moves compared to the baseline optimal strategy"
/>

### Evaluation

Now that the model is trained, I can evaluate its performance and its value function.

#### Performance

Below I compare policies and their reward distributions. To do this, I play $1,000$ games of $100$ rounds of blackjack, for each policy. For each of these games, I can find the average reward per hand per round, and plot the distribution of these.

<Table 
  header={["Policy", "Description"]}
  rows={[
    ["Learned", "the learned policy from training"],
    ["Accepted", "baseline optimal policy found online"],
    ["Random", "completely random policy"],
    ["Mimic House", "stay on anything higher than 16, hit otherwise"]
  ]}
  caption="4 policies evaluated"
/>

<MdImage
  src="/images/posts/q-learning-comparison.png"
  caption="Rewards distribution for each policy"
/>

While we have a rewards distribution, we can can capture the expected value of reward per round, in terms of units, for each policy.
- **Learned**: -0.0082
- **Accepted**: -0.0094
- **Random**: -0.4243
- **Mimic House**: -0.0628

#### Value Function

Next, I can determine the value function to be able to visualize our state values.

$V_{\pi}(s)$ tells us, according to a policy $\pi$, what the "value" is of being in state $s$. We can relate this back to $Q_{\pi}(s,a)$, as I'll show below. First, we have the equations for $V_{\pi}(s)$ and $Q_{\pi}(s,a)$:
```math
\begin{align*}
V_{\pi}(s) &= E_{\pi}\left[\sum_{t=0}^{T}\gamma^tr_t | s_t = s\right]\\
Q_{\pi}(s,a) &= E_{\pi}\left[\sum_{t=0}^{T}\gamma^tr_t | s_t = s, a_t=a\right]
\end{align*}
```
We can relate $V$ and $Q$ by taking marginal expections over $a$
```math
\begin{align*}
V_{\pi}(s) &= E_{\pi}\left[\sum_{t=0}^{T}\gamma^tr_t | s_t = s\right]\\
V_{\pi}(s) &= E_a\left[E_{\pi}\left[\sum_{t=0}^{T}\gamma^tr_t | s_t = s, a_t=a\right] \right]\\
V_{\pi}(s) &= E_a\left[Q_{\pi}(s,a)\right]\\
V_{\pi}(s) &= \sum_{a}\pi(s,a)Q(s,a)
\end{align*}
```

Where $\pi(s,a)$ is the probability of taking action $a$ in state $s$. In our case, $\pi(s,a)$ will equal 1 if the action is equal to $max(Q(s,a))$, and $0$ otherwise, since we don't have transition probabilities and assume we always take the optimal action during inference, which leads to the following:
```math
\begin{align*}
V_s &= max(Q(s,a))
\end{align*}
```
Performing this for all state-action pairs, partitioned by <mark>Hard Total, Soft Total, Can Split</mark> results in the following:

<MdImage
  src="/images/posts/q-learning-value-fct.png"
  caption="Value function for each state-action pair, partitioned by hand type."
/>

It's quite neat seeing these value functions arise. I think for the most part they're rather intuitive. We exclude player values of 21 from the analysis, as the action-space is empty.

## Additional Analyses

There are so many additional analyses that can be done having created the Blackjack gameplay modules, outlined the blackjack environment, and trained the RL agent.

A lot of additional analyses is conducted in the notebook on github, but here I want to highlight the impact of bankroll, which I think is a really interesting component to dive into. A trained model isn't necessarily required to conduct these, but I'll use mine to get results empirically.

Let's assume you wager 1 unit every round, as that is the table minimum. Let's evaluate the impact of your total bankroll, using the same exact policy to play the game of blackjack.

<MdImage
  src="/images/posts/q-learning-bankroll-p2.png"
  caption="Probability of being bankrolled in < N hands. All of these agents follow the same policy to generate the data."
/>

Lastly, I found it quite interesting to assess how your expected value changes according to your starting bankroll.

<MdImage
  src="/images/posts/q-learning-bankroll-p1.png"
  caption="As your bankroll decreases, your profitability per hand decreases as well."
/>

It's a classic example of bankroll across various games. No matter what policy you follow, or how adept of a player you are, if you have a smaller bankroll, you are at a disadvantage. At least following a more optimal policy will help mitigate that risk, but only so much. Simply by chance, you might lose enough consecutive hands that would cause you be out of money. This is why we see drastic drops in EV compared to different bankrolls of players, despite following the same exact policy.

<UrlPreview url="https://github.com/petersim1/Blackjack_RL" landscape={true} />
